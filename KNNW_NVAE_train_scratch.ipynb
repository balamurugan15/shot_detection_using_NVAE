{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNNW_NVAE_train_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "j-ktjX7jj0nA",
        "n5gi4AQ8h6Ks",
        "acvXdpd2h1RZ",
        "5KOJbv-chnHF",
        "xEf0wLQHhq38"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "73d8ef951042457b927613ec17e591ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_250afcb3da48428f88cab118f9d47a39",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_96c416293dba41c4964b5763780dcec1",
              "IPY_MODEL_30a394c6187c4427978d0c2daf11adfd",
              "IPY_MODEL_79f6333d4be340139200c3ddd290ca84"
            ]
          }
        },
        "250afcb3da48428f88cab118f9d47a39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96c416293dba41c4964b5763780dcec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d639dc9cab0a45eeb849aea98b3c8913",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_51200ee716a74adfacda24b3a39339e4"
          }
        },
        "30a394c6187c4427978d0c2daf11adfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_79eb2901e58d4df0b2145e1fcfe83e16",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3c26c2a2bbfc460185b2432c090b2172"
          }
        },
        "79f6333d4be340139200c3ddd290ca84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f7f71e03cabf4e96b457af3b10423c43",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/? [01:40&lt;00:00,  1.41s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_489e2f6debda4142961ab0efe83c9632"
          }
        },
        "d639dc9cab0a45eeb849aea98b3c8913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "51200ee716a74adfacda24b3a39339e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "79eb2901e58d4df0b2145e1fcfe83e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3c26c2a2bbfc460185b2432c090b2172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "20px",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f7f71e03cabf4e96b457af3b10423c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "489e2f6debda4142961ab0efe83c9632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n",
        "\n",
        "# This work is licensed under the NVIDIA Source Code License\n",
        "# for NVAE. To view a copy of this license, see the LICENSE file.\n",
        "# ---------------------------------------------------------------"
      ],
      "metadata": {
        "id": "ORYhhhz2npL7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmvVJQnRcUHw",
        "outputId": "bf151e7c-e89e-4be8-c3d0-e462b702ee79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec 11 00:13:32 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Research_copy/knnw/vae/NVAE"
      ],
      "metadata": {
        "id": "RfG3xlRKc0gS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49069f12-6375-494d-c758-a54fd5f0391e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Research_copy/knnw/vae/NVAE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "si-tFs5otA3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "j-ktjX7jj0nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "\n",
        "import torch.distributed as dist\n",
        "from torch.multiprocessing import Process\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from model import AutoEncoder\n",
        "from thirdparty.adamax import Adamax\n",
        "import utils\n",
        "import datasets\n",
        "\n",
        "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "os.environ['MASTER_PORT'] = '6020'\n",
        "\n",
        "torch.distributed.init_process_group(backend='nccl', init_method='env://', rank=0, world_size=1)\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "import urllib\n",
        "from lmdb_datasets import LMDBDataset\n",
        "from thirdparty.lsun import LSUN\n",
        "\n",
        "from sklearn.model_selection import train_test_split "
      ],
      "metadata": {
        "id": "76KHMdXGc-oa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset and Dataloaders"
      ],
      "metadata": {
        "id": "n5gi4AQ8h6Ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loaders(args):\n",
        "    \"\"\"Get data loaders for required dataset.\"\"\"\n",
        "    return get_loaders_eval(args.dataset, args)\n",
        "\n",
        "class MyImageFolder(dset.ImageFolder):\n",
        "    def __getitem__(self, index):\n",
        "        return super(MyImageFolder, self).__getitem__(index) #return image path\n",
        "\n",
        "\n",
        "class KNNWDataset(Dataset):\n",
        "  def __init__(self,X, transform_train):\n",
        "    X = np.load(X, allow_pickle=True)\n",
        "    train_data, _ = train_test_split(X, test_size=0.2, shuffle=False)\n",
        "    self.x = train_data\n",
        "    self.transform = transform_train\n",
        "    # self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    x = self.x[index]\n",
        "    # x = x/255\n",
        "    x = self.transform(x).permute(1,2,0)\n",
        "    return (x, 0)\n",
        "\n",
        "\n",
        "\n",
        "def get_loaders_eval(dataset, args):\n",
        "    \"\"\"Get train and valid loaders for cifar10/tiny imagenet.\"\"\"\n",
        "\n",
        "    if dataset == 'cifar10':\n",
        "        num_classes = 10\n",
        "        train_transform, valid_transform = _data_transforms_cifar10(args)\n",
        "        train_data = dset.CIFAR10(\n",
        "            root=args.data, train=True, download=True, transform=train_transform)\n",
        "        valid_data = dset.CIFAR10(\n",
        "            root=args.data, train=False, download=True, transform=valid_transform)\n",
        "      #-----------------------------------\n",
        "    elif dataset.startswith('knnw'):\n",
        "        num_classes = 0\n",
        "\n",
        "        train_transform, _ = _data_transforms_knnw()\n",
        "\n",
        "        train_data = KNNWDataset(args.data, train_transform)\n",
        "        valid_data = train_data\n",
        "        \n",
        "      #--------------------------------------------------\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    train_sampler, valid_sampler = None, None\n",
        "    if args.distributed:\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)\n",
        "        valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_data)\n",
        "\n",
        "    train_queue = torch.utils.data.DataLoader(\n",
        "        train_data, batch_size=args.batch_size,\n",
        "        shuffle=(train_sampler is None),\n",
        "        sampler=train_sampler, pin_memory=True, num_workers=8, drop_last=True)\n",
        "\n",
        "    valid_queue = torch.utils.data.DataLoader(\n",
        "        valid_data, batch_size=args.batch_size,\n",
        "        shuffle=(valid_sampler is None),\n",
        "        sampler=valid_sampler, pin_memory=True, num_workers=1, drop_last=False)\n",
        "\n",
        "    return train_queue, valid_queue, num_classes\n",
        "\n",
        "\n",
        "def _data_transforms_cifar10(args):\n",
        "    \"\"\"Get data transforms for cifar10.\"\"\"\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    valid_transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    return train_transform, valid_transform\n",
        "\n",
        "\n",
        "def _data_transforms_generic(size):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    valid_transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    return train_transform, valid_transform\n",
        "\n",
        "    #----------------------------\n",
        "def _data_transforms_knnw():\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "    ])\n",
        "\n",
        "    valid_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    return train_transform, valid_transform\n",
        "\n",
        "#---------------------------------------"
      ],
      "metadata": {
        "id": "UUm_EL54h5gk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dumD9vOqlWln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model initialization"
      ],
      "metadata": {
        "id": "acvXdpd2h1RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class args:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.ada_groups=False\n",
        "        self.arch_instance='res_mbconv'\n",
        "        self.batch_size=2\n",
        "        self.cont_training=False\n",
        "        self.data='/content/unique_32x32.npy'\n",
        "        self.dataset='knnw'\n",
        "        self.epochs=400\n",
        "        self.fast_adamax=True\n",
        "        self.global_rank=0\n",
        "        self.kl_anneal_portion=0.3\n",
        "        self.kl_const_coeff=0.0001\n",
        "        self.kl_const_portion=0.0001\n",
        "        self.learning_rate=0.01\n",
        "        self.learning_rate_min=0.0001\n",
        "        self.local_rank=0\n",
        "        self.master_address='127.0.0.1'\n",
        "        self.min_groups_per_scale=1\n",
        "        self.node_rank=0\n",
        "        self.num_cell_per_cond_dec=2\n",
        "        self.num_cell_per_cond_enc=2\n",
        "        self.num_channels_dec=32\n",
        "        self.num_channels_enc=32\n",
        "        self.num_groups_per_scale=30\n",
        "        self.num_latent_per_group=20\n",
        "        self.num_latent_scales=1\n",
        "        self.num_mixture_dec=10\n",
        "        self.num_nf=1\n",
        "        self.num_postprocess_blocks=1\n",
        "        self.num_postprocess_cells=2\n",
        "        self.num_preprocess_blocks=1\n",
        "        self.num_preprocess_cells=2\n",
        "        self.num_proc_node=1\n",
        "        self.num_process_per_node=8\n",
        "        self.num_x_bits=8\n",
        "        self.res_dist=True\n",
        "        self.root='PATH_TO_CHECKPOINT_DIR'\n",
        "        self.save='UNIQUE_EXPR_ID'\n",
        "        self.seed=1\n",
        "        self.use_se=False\n",
        "        self.warmup_epochs=5\n",
        "        self.weight_decay=0.0003\n",
        "        self.weight_decay_norm_anneal=False\n",
        "        self.weight_decay_norm=0.01\n",
        "        self.distributed = False\n",
        "\n",
        "args = args()\n",
        "writer = None\n",
        "arch_instance = utils.get_arch_cells(args.arch_instance)\n"
      ],
      "metadata": {
        "id": "V6Ck7Rejh0tW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDUrUj582d76",
        "outputId": "aa19037f-66a9-457c-e534-429a9d80feb3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(args.root)\n",
        "os.mkdir(args.save)\n",
        "os.mkdir(args.data)"
      ],
      "metadata": {
        "id": "RrTRiYsZ3ip7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1iQu3EzT1VHUZnCyBvYXIGom2_V0Z79wg &     #Unique Label Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTHjsJPP2Num",
        "outputId": "20f0d253-5976-44ba-f5b7-bc409e922bec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1iQu3EzT1VHUZnCyBvYXIGom2_V0Z79wg\n",
            "To: /content/knnw_720p_qscale31_unique.tar.gz\n",
            "100% 2.08G/2.08G [00:26<00:00, 77.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf \"/content/knnw_720p_qscale31_unique.tar.gz\" -C \"/content\"     #Unzip Unique Label Data"
      ],
      "metadata": {
        "id": "SRHrB8fO2hl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1ZqUAj6yZItzFG9b4OJdnEEwSumUMEuVO #unique_32x32.npy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K7Rujfm4pMU",
        "outputId": "e2c8ed07-2f4a-4676-b14c-94e822c63b1f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZqUAj6yZItzFG9b4OJdnEEwSumUMEuVO\n",
            "To: /content/unique_32x32.npy\n",
            "100% 336M/336M [00:02<00:00, 119MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir val_data"
      ],
      "metadata": {
        "id": "qzHcid6t2ltm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv qscale31_unique val_data/."
      ],
      "metadata": {
        "id": "GDFBgeJs2pBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "logging = utils.Logger(args.global_rank, args.save)\n",
        "writer = utils.Writer(args.global_rank, args.save)\n",
        "\n",
        "# Get data loaders.\n",
        "print(args.batch_size)\n",
        "train_queue, valid_queue, num_classes = datasets.get_loaders(args)\n",
        "args.num_total_iter = len(train_queue) * args.epochs\n",
        "warmup_iters = len(train_queue) * args.warmup_epochs\n",
        "swa_start = len(train_queue) * (args.epochs - 1)\n",
        "\n",
        "arch_instance = utils.get_arch_cells(args.arch_instance)\n",
        "\n",
        "model = AutoEncoder(args, writer, arch_instance)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "DKvji4Zyelc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "658a1f8f-2cdc-48de-c94d-07743e4a33e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "len log norm: 610\n",
            "len bn: 364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training loop"
      ],
      "metadata": {
        "id": "5KOJbv-chnHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_queue, model, cnn_optimizer, grad_scalar, global_step, warmup_iters, writer, args, logging, cnn_scheduler, checkpoint_file, arch_instance):\n",
        "    alpha_i = utils.kl_balancer_coeff(num_scales=model.num_latent_scales,\n",
        "                                      groups_per_scale=model.groups_per_scale, fun='square')\n",
        "    nelbo = utils.AvgrageMeter()\n",
        "    model.train()\n",
        "    for step, x in tqdm(enumerate(train_queue)):\n",
        "        # print('Length X', len(x))\n",
        "        x = x[0] if len(x) > 1 else x\n",
        "        x = x.cuda()\n",
        "        # print('X shape', x.shape)\n",
        "\n",
        "        # change bit length\n",
        "        x = utils.pre_process(x, args.num_x_bits)\n",
        "\n",
        "        # warm-up lr\n",
        "        if global_step < warmup_iters:\n",
        "            lr = args.learning_rate * float(global_step) / warmup_iters\n",
        "            for param_group in cnn_optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "        # sync parameters, it may not be necessary\n",
        "        if step % 100 == 0:\n",
        "            utils.average_params(model.parameters(), args.distributed)\n",
        "\n",
        "        cnn_optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            logits, log_q, log_p, kl_all, kl_diag = model(x)\n",
        "\n",
        "            output = model.decoder_output(logits)\n",
        "            kl_coeff = utils.kl_coeff(global_step, args.kl_anneal_portion * args.num_total_iter,\n",
        "                                      args.kl_const_portion * args.num_total_iter, args.kl_const_coeff)\n",
        "\n",
        "            recon_loss = utils.reconstruction_loss(output, x, crop=model.crop_output)\n",
        "            balanced_kl, kl_coeffs, kl_vals = utils.kl_balancer(kl_all, kl_coeff, kl_balance=True, alpha_i=alpha_i)\n",
        "\n",
        "            nelbo_batch = recon_loss + balanced_kl\n",
        "            loss = torch.mean(nelbo_batch)\n",
        "            norm_loss = model.spectral_norm_parallel()\n",
        "            bn_loss = model.batchnorm_loss()\n",
        "            # get spectral regularization coefficient (lambda)\n",
        "            if args.weight_decay_norm_anneal:\n",
        "                assert args.weight_decay_norm_init > 0 and args.weight_decay_norm > 0, 'init and final wdn should be positive.'\n",
        "                wdn_coeff = (1. - kl_coeff) * np.log(args.weight_decay_norm_init) + kl_coeff * np.log(args.weight_decay_norm)\n",
        "                wdn_coeff = np.exp(wdn_coeff)\n",
        "            else:\n",
        "                wdn_coeff = args.weight_decay_norm\n",
        "\n",
        "            loss += norm_loss * wdn_coeff + bn_loss * wdn_coeff\n",
        "\n",
        "        grad_scalar.scale(loss).backward()\n",
        "        utils.average_gradients(model.parameters(), args.distributed)\n",
        "        del x, balanced_kl, kl_coeffs, kl_vals, logits, log_q, log_p, kl_all, kl_diag\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        grad_scalar.step(cnn_optimizer)\n",
        "        grad_scalar.update()\n",
        "        nelbo.update(loss.data, 1)\n",
        "\n",
        "\n",
        "\n",
        "        if (global_step + 1) % 100 == 0:\n",
        "\n",
        "            logging.info('saving the model.')\n",
        "            torch.save({'epoch': 100, 'state_dict': model.state_dict(),\n",
        "                    'optimizer': cnn_optimizer.state_dict(), 'global_step': global_step,\n",
        "                    'args': args, 'arch_instance': arch_instance, 'scheduler': cnn_scheduler.state_dict(),\n",
        "                    'grad_scalar': grad_scalar.state_dict()}, checkpoint_file + \"2\")\n",
        "\n",
        "            logging.info('train %d %f', global_step, nelbo.avg)\n",
        "\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "    utils.average_tensor(nelbo.avg, args.distributed)\n",
        "    return nelbo.avg, global_step"
      ],
      "metadata": {
        "id": "kE5hd6YTfJFG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run Training"
      ],
      "metadata": {
        "id": "xEf0wLQHhq38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ensures that weight initializations are all the same\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "logging = utils.Logger(args.global_rank, args.save)\n",
        "\n",
        "# Get data loaders.\n",
        "train_queue, valid_queue, num_classes = datasets.get_loaders(args)\n",
        "args.num_total_iter = len(train_queue) * args.epochs\n",
        "warmup_iters = len(train_queue) * args.warmup_epochs\n",
        "swa_start = len(train_queue) * (args.epochs - 1)\n",
        "\n",
        "arch_instance = utils.get_arch_cells(args.arch_instance)\n",
        "\n",
        "model = AutoEncoder(args, writer, arch_instance)\n",
        "model = model.cuda()\n",
        "\n",
        "logging.info('args = %s', args)\n",
        "logging.info('param size = %fM ', utils.count_parameters_in_M(model))\n",
        "logging.info('groups per scale: %s, total_groups: %d', model.groups_per_scale, sum(model.groups_per_scale))\n",
        "\n",
        "if args.fast_adamax:\n",
        "  # Fast adamax has the same functionality as torch.optim.Adamax, except it is faster.\n",
        "  cnn_optimizer = Adamax(model.parameters(), args.learning_rate,\n",
        "                          weight_decay=args.weight_decay, eps=1e-3)\n",
        "else:\n",
        "  cnn_optimizer = torch.optim.Adamax(model.parameters(), args.learning_rate,\n",
        "                                      weight_decay=args.weight_decay, eps=1e-3)\n",
        "\n",
        "cnn_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "  cnn_optimizer, float(args.epochs - args.warmup_epochs - 1), eta_min=args.learning_rate_min)\n",
        "grad_scalar = GradScaler(2**10)\n",
        "\n",
        "num_output = utils.num_output(args.dataset)\n",
        "bpd_coeff = 1. / np.log(2.) / num_output\n",
        "\n",
        "# if load\n",
        "checkpoint_file = os.path.join(args.save, 'checkpoint.pt')\n",
        "if args.cont_training:\n",
        "  logging.info('loading the model.')\n",
        "  checkpoint = torch.load(checkpoint_file, map_location='cpu')\n",
        "  init_epoch = checkpoint['epoch']\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  model = model.cuda()\n",
        "  cnn_optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  grad_scalar.load_state_dict(checkpoint['grad_scalar'])\n",
        "  cnn_scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "  global_step = checkpoint['global_step']\n",
        "else:\n",
        "  global_step, init_epoch = 0, 0\n",
        "\n",
        "for epoch in range(init_epoch, args.epochs):\n",
        "  # update lrs.\n",
        "  if args.distributed:\n",
        "      train_queue.sampler.set_epoch(global_step + args.seed)\n",
        "      valid_queue.sampler.set_epoch(0)\n",
        "\n",
        "  if epoch > args.warmup_epochs:\n",
        "      cnn_scheduler.step()\n",
        "\n",
        "  # Logging.\n",
        "  logging.info('epoch %d', epoch)\n",
        "\n",
        "  # Training.\n",
        "  train_nelbo, global_step = train(train_queue, model, cnn_optimizer, grad_scalar, global_step, warmup_iters, writer, args, logging, cnn_scheduler, checkpoint_file, arch_instance)\n",
        "  logging.info('train_nelbo %f', train_nelbo)\n",
        "\n",
        "  model.eval()\n",
        "  # generate samples less frequently\n",
        "  eval_freq = 1 if args.epochs <= 50 else 20\n",
        "  if epoch % eval_freq == 0 or epoch == (args.epochs - 1):\n",
        "      with torch.no_grad():\n",
        "          num_samples = 16\n",
        "          n = int(np.floor(np.sqrt(num_samples)))\n",
        "          for t in [0.7, 0.8, 0.9, 1.0]:\n",
        "              logits = model.sample(num_samples, t)\n",
        "              output = model.decoder_output(logits)\n",
        "              output_img = output.mean if isinstance(output, torch.distributions.bernoulli.Bernoulli) else output.sample(t)\n",
        "              output_tiled = utils.tile_image(output_img, n)\n",
        "\n",
        "      valid_neg_log_p, valid_nelbo = test(valid_queue, model, num_samples=10, args=args, logging=logging)\n",
        "      logging.info('valid_nelbo %f', valid_nelbo)\n",
        "      logging.info('valid neg log p %f', valid_neg_log_p)\n",
        "      logging.info('valid bpd elbo %f', valid_nelbo * bpd_coeff)\n",
        "      logging.info('valid bpd log p %f', valid_neg_log_p * bpd_coeff)\n",
        "\n",
        "  save_freq = int(np.ceil(args.epochs / 100))\n",
        "  if epoch % save_freq == 0 or epoch == (args.epochs - 1):\n",
        "      if args.global_rank == 0:\n",
        "          logging.info('saving the model.')\n",
        "          torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
        "                      'optimizer': cnn_optimizer.state_dict(), 'global_step': global_step,\n",
        "                      'args': args, 'arch_instance': arch_instance, 'scheduler': cnn_scheduler.state_dict(),\n",
        "                      'grad_scalar': grad_scalar.state_dict()}, checkpoint_file)\n",
        "\n",
        "# Final validation\n",
        "valid_neg_log_p, valid_nelbo = test(valid_queue, model, num_samples=1000, logging=logging)\n",
        "logging.info('final valid nelbo %f', valid_nelbo)\n",
        "logging.info('final valid neg log p %f', valid_neg_log_p)"
      ],
      "metadata": {
        "id": "irSJTlE2eyJ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "73d8ef951042457b927613ec17e591ad",
            "250afcb3da48428f88cab118f9d47a39",
            "96c416293dba41c4964b5763780dcec1",
            "30a394c6187c4427978d0c2daf11adfd",
            "79f6333d4be340139200c3ddd290ca84",
            "d639dc9cab0a45eeb849aea98b3c8913",
            "51200ee716a74adfacda24b3a39339e4",
            "79eb2901e58d4df0b2145e1fcfe83e16",
            "3c26c2a2bbfc460185b2432c090b2172",
            "f7f71e03cabf4e96b457af3b10423c43",
            "489e2f6debda4142961ab0efe83c9632"
          ]
        },
        "outputId": "0d5fa8d3-af06-4ac4-e3af-da0617030bb5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len log norm: 610\n",
            "len bn: 364\n",
            "12/11 12:44:30 AM (Elapsed: 00:00:01) args = <__main__.args object at 0x7f35bd582850>\n",
            "12/11 12:44:30 AM (Elapsed: 00:00:01) param size = 10.823000M \n",
            "12/11 12:44:30 AM (Elapsed: 00:00:01) groups per scale: [30], total_groups: 30\n",
            "12/11 12:44:30 AM (Elapsed: 00:00:01) epoch 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73d8ef951042457b927613ec17e591ad",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-99c85fe82471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   \u001b[0mtrain_nelbo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_scalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_nelbo %f'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_nelbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-8dfe654948e7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_queue, model, cnn_optimizer, grad_scalar, global_step, warmup_iters, writer, args, logging, cnn_scheduler, checkpoint_file, arch_instance)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcnn_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_diag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Research_copy/knnw/vae/NVAE/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    403\u001b[0m                     \u001b[0mmu_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_sig_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_p\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmu_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_sig_p\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlog_sig_q\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_dist\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_sig_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                     \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m                     \u001b[0mlog_q_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                     \u001b[0;31m# apply NF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Research_copy/knnw/vae/NVAE/distributions.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msample_normal_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_given_eps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation\n",
        "\n",
        "Run evaluation using knnw_nvae_eval.ipynb"
      ],
      "metadata": {
        "id": "OzfgO2xHjXq_"
      }
    }
  ]
}